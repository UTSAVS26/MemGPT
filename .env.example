##########################################################
# Example environment variable configurations for the Letta
# Docker container. Uncomment the sections you want to
# configure.
#
# Hint: You don't need to have the same LLM and embedding 
# model backends; you can mix and match.
##########################################################


##########################################################
#                OpenAI Configuration
##########################################################

# LLM Model Configuration
# Uncomment the following lines to use OpenAI as the LLM backend
# LETTA_LLM_ENDPOINT_TYPE=openai  # Specify the type of LLM endpoint (OpenAI)
# LETTA_LLM_MODEL=gpt-4o-mini      # Specify the LLM model to use (e.g., GPT-4 variant)

# Embeddings Configuration
# Uncomment the following lines to use OpenAI for embeddings
# LETTA_EMBEDDING_ENDPOINT_TYPE=openai  # Specify the type of embedding endpoint (OpenAI)
# LETTA_EMBEDDING_MODEL=text-embedding-ada-002  # Specify the embedding model to use


##########################################################
#                Ollama Configuration
##########################################################

# LLM Model Configuration
# Uncomment the following lines to use Ollama as the LLM backend
# LETTA_LLM_ENDPOINT=http://host.docker.internal:11434  # Set the Ollama server endpoint
# LETTA_LLM_ENDPOINT_TYPE=ollama  # Specify the type of LLM endpoint (Ollama)
# LETTA_LLM_MODEL=dolphin2.2-mistral:7b-q6_K  # Specify the LLM model to use
# LETTA_LLM_CONTEXT_WINDOW=8192  # Set the context window size for the model

# Embeddings Configuration
# Uncomment the following lines to use Ollama for embeddings
# LETTA_EMBEDDING_ENDPOINT=http://host.docker.internal:11434  # Set the Ollama server endpoint for embeddings
# LETTA_EMBEDDING_ENDPOINT_TYPE=ollama  # Specify the type of embedding endpoint (Ollama)
# LETTA_EMBEDDING_MODEL=mxbai-embed-large  # Specify the embedding model to use
# LETTA_EMBEDDING_DIM=512  # Set the dimensionality of the embeddings


##########################################################
#                vLLM Configuration
##########################################################

# LLM Model Configuration
# Uncomment the following lines to use vLLM as the LLM backend
# LETTA_LLM_ENDPOINT=http://host.docker.internal:8000  # Set the vLLM server endpoint
# LETTA_LLM_ENDPOINT_TYPE=vllm  # Specify the type of LLM endpoint (vLLM)
# LETTA_LLM_MODEL=ehartford/dolphin-2.2.1-mistral-7b  # Specify the LLM model to use
# LETTA_LLM_CONTEXT_WINDOW=8192  # Set the context window size for the model
